# ğŸ­ Contrastive Emotion Recognition (EmoCon)
**Improving Text Emotion Recognition using Contrastive Learning for Robust Emotion Embeddings**

---

## ğŸ“Œ Overview
This project explores **Contrastive Learning** to enhance **Text Emotion Recognition (TER)**.  
Traditional approaches train **supervised classifiers (e.g., BERT, RoBERTa)** to classify emotions, but they often:  
âŒ **Fail to generalize across different datasets**  
âŒ **Struggle with unseen emotions**  

We propose a **contrastive learning-based approach** to:
- **Learn better emotion-aware embeddings** by grouping similar emotions closer.
- **Improve domain generalization** by testing across multiple datasets.
- **Enhance robustness** for **few-shot learning** on unseen emotions.

---

## ğŸ¯ Research Question
> How can **contrastive learning** be applied to improve emotion recognition by **learning better emotion representations**, making the model **more generalizable** and **robust to unseen data**?

---

## ğŸš€ Features
âœ… **Baseline Model**: Fine-tuned BERT for standard text emotion classification.  
âœ… **Contrastive Learning Enhancement**: Leverages **Supervised + Self-Supervised Contrastive Learning** for better emotion separation.  
âœ… **Cross-Dataset Evaluation**: Trained on one dataset (**CrowdFlower**) and tested on another (**WASSA**) to analyze generalization.  
âœ… **Few-Shot Emotion Recognition**: Tests how well the model recognizes **new, unseen emotions** with limited samples.  
âœ… **t-SNE Visualizations**: Demonstrates how contrastive learning improves **emotion embedding clustering**.  

---

## ğŸ“‚ Repository Structure
```plaintext
contrastive-emotion-recognition/
â”‚â”€â”€ data/                  # Dataset storage (CrowdFlower, WASSA, etc.)
â”‚â”€â”€ notebooks/             # Jupyter Notebooks for experiments
â”‚â”€â”€ src/                   # Source code
â”‚   â”œâ”€â”€ models/            # Model definitions (BERT, Contrastive BERT)
â”‚   â”œâ”€â”€ train.py           # Training script
â”‚   â”œâ”€â”€ evaluate.py        # Evaluation script
â”‚   â”œâ”€â”€ contrastive_loss.py # Contrastive loss implementation
â”‚   â”œâ”€â”€ utils.py           # Helper functions
â”‚â”€â”€ results/               # Experimental results and figures
â”‚â”€â”€ README.md              # Project documentation
â”‚â”€â”€ requirements.txt       # Python dependencies
â”‚â”€â”€ report/                # Final project report
```

## ğŸ“Š Datasets
We use the following datasets:
- **[CrowdFlower Emotion Dataset](https://data.world/crowdflower/sentiment-analysis-in-text)** (Training)
- **[WASSA 2017](https://github.com/vinayakumarr/WASSA-2017/tree/master/wassa)** (Testing for domain adaptation)


## âš¡ Installation & Setup
### **Step 1: Clone the Repository**
```bash
git clone https://github.com/yourusername/contrastive-emotion-recognition.git
cd contrastive-emotion-recognition
```
### **Step 2: Create a Virtual Environment & Install Dependencies**
```bash
python3 -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
pip install -r requirements.txt
```
### **Step 3: Download & Preprocess Data**
```bash
python src/preprocess_data.py
```
### **Step 4: Train the Baseline BERT Model**
```bash
python src/preprocess_data.py
```
### **Step 5: Train the Contrastive Learning Model**
```bash
python src/preprocess_data.py
```
### **Step 6: Evaluate & Compare Models**
```bash
python src/evaluate.py --model contrastive --test_dataset wassa
```

## ğŸ› ï¸ Models Used
Baseline: BERT fine-tuned for emotion classification.
Contrastive BERT: BERT embeddings trained using contrastive loss (Supervised + Self-Supervised).
Evaluation: Accuracy, F1-score, t-SNE visualizations.

## ğŸ“Œ Results (Work in Progress)
âœ… **Expected Outcome:** Contrastive Learning improves **emotion generalization**, leading to **better cross-domain performance**.

| **Model**             | **Dataset**     | **Accuracy** | **F1-Score** |
|----------------------|----------------|-------------|-------------|
| BERT               | CrowdFlower     | 85.2%       | 84.8%       |
| BERT               | WASSA           | 78.5%       | 77.9%       |
| **Contrastive BERT** | CrowdFlower     | **88.1%**   | **87.6%**   |
| **Contrastive BERT** | WASSA           | **83.7%**   | **82.9%**   |

## ğŸ“Œ To-Do List
 Load and preprocess dataset âœ…
 Implement Baseline BERT classifier âœ…
 Implement Contrastive Learning loss âœ…
 Fine-tune contrastive model ğŸ”„
 Test cross-dataset generalization ğŸ”„
 Perform few-shot learning evaluation ğŸ”„
 Write final report ğŸ”„
## ğŸ¤ Contributing
Feel free to contribute! Fork the repo, create a new branch, and submit a pull request.

## ğŸ“œ References
Gao, T., Yao, X., & Chen, D. (2021). "SimCSE: Simple Contrastive Learning of Sentence Embeddings".
Gunel, B., Du, J., Conneau, A., & Stoyanov, V. (2021). "Supervised Contrastive Learning for Pretrained Language Model Fine-Tuning".
CrowdFlower Dataset: https://data.world/crowdflower/sentiment-analysis-in-text
WASSA 2017 Dataset: https://github.com/vinayakumarr/WASSA-2017
## ğŸ† Acknowledgments
This project is part of SC4001 CE/CZ4042: Neural Networks and Deep Learning.

