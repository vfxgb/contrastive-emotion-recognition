# ğŸ­ Contrastive-Mamba Emotion Recognition (EmoCon)
**Enhancing Text Emotion Recognition with Mamba and Contrastive Learning for Robust Emotion Embeddings**

---

## ğŸ“Œ Overview
This project integrates the powerful **Selective State Space Model (Mamba)** with **Contrastive Learning** to enhance **Text Emotion Recognition (TER)**.  
Traditional models (e.g., CNN, RNN, BERT) often:
- âŒ **Fail to capture global sentence context effectively**.
- âŒ **Struggle with cross-dataset generalization and unseen emotions**.

We propose an innovative **Mamba-Contrastive approach** that:
- Captures both **local (word-level)** and **global (sentence-level)** emotional contexts using **Selective Mamba**.
- Learns robust, generalizable emotion-aware embeddings through **Contrastive Learning**, enhancing performance on new, unseen data.

---

## ğŸ¯ Research Question
> How can integrating **Selective State Space Models (Mamba)** with **Contrastive Learning** significantly improve **emotion embedding quality**, **domain generalization**, and robustness to **unseen emotions**?

---

## ğŸš€ Key Features
âœ… **Baseline Model:** Fine-tuned BERT for standard emotion classification.  
âœ… **Proposed Model:** **Selective Mamba + Contrastive Learning** to leverage global context and learn robust emotion embeddings.  
âœ… **Cross-Dataset Evaluation:** Train on **CrowdFlower**, evaluate on **WASSA 2021** & **ISEAR** to test generalization.  

---

## ğŸ“‚ Updated Repository Structure - To DO
```plaintext
contrastive-emotion-recognition/
â”œâ”€â”€ data/                           # Datasets: CrowdFlower, ISEAR, WASSA2021
â”‚   â”œâ”€â”€ CrowdFlower/
â”‚   â”œâ”€â”€ ISEAR/
â”‚   â””â”€â”€ WASSA2021/
â”‚   â””â”€â”€ README.md                   # Short note about each dataset
â”œâ”€â”€ notebooks/                      # (Optional) Jupyter notebooks for exploration
â”œâ”€â”€ report/                         # Final project report (replaces "theory")
â”œâ”€â”€ results/                        # Experimental results, logs, etc.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ mamba.py               # Mamba model code
â”‚   â”‚   â””â”€â”€ contrastive_model.py   # (Or BILSTM modules, etc.)
â”‚   â”œâ”€â”€ preprocess/
â”‚   â”‚   â”œâ”€â”€ preprocess_crowdflower.py
â”‚   â”‚   â”œâ”€â”€ preprocess_isear.py
â”‚   â”‚   â””â”€â”€ preprocess_wassa2021.py
â”‚   â”œâ”€â”€ train_and_test_bilstm_bert/
â”‚   â”œâ”€â”€ train_and_test_bilstm_glove/
â”‚   â”œâ”€â”€ train_and_test_mamba/
â”‚   â”œâ”€â”€ contrastive_loss.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ train.py                   # Consolidated training script (optional)
â”‚   â””â”€â”€ evaluate.py                # Consolidated evaluation script (optional)
â”œâ”€â”€ environment.yaml               # Conda environment (if you keep using Conda)
â”œâ”€â”€ requirements.txt               # Pip dependencies (optional or if you prefer pip)
â”œâ”€â”€ run_pipeline.sh                # Main pipeline script
â”œâ”€â”€ job_run_pipeline.sh            # SLURM batch script for HPC/cluster
â”œâ”€â”€ setup.py                       # For installing as a Python package (optional)
â””â”€â”€ README.md                      # Project overview, usage, and instructions

```

---

## ğŸ“Š Datasets Used
- **CrowdFlower Emotion Dataset** (Training)
- **WASSA 2021** (Domain generalization testing)
- **ISEAR** (Domain generalization testing)

---

## âš¡ Installation & Setup
### **Step 1: Clone the Repository**
```bash
git clone https://github.com/yourusername/contrastive-emotion-recognition.git
cd contrastive-emotion-recognition.git
```

### **Step 2: Create a Virtual Environment & Install Dependencies**
```bash
conda env create -f environment.yaml
conda activate SC4001
```
### **Step 3: Run the Pipeline**

| Command                                                                                        | Description                                                                                                      | Example                                                                                           |
|------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|
| `./run_pipeline.sh <dataset> <model>`                                                          | Runs preprocessing and train/test for the given dataset and model.                                               | `./run_pipeline.sh crowdflower mamba`                                                              |
| `./run_pipeline.sh --force_preprocess <dataset> <model>`                                       | Forces re-preprocessing (even if already done) and then runs train/test.                                         | `./run_pipeline.sh --force_preprocess crowdflower bilstm_glove`                                    |
| `./run_pipeline.sh <dataset> <model> --finetune_mode <1\|2\|3>`                                | Runs with a specified finetuning strategy (only required when dataset is `isear` or `wassa` + model is `mamba` or `bilstm_bert`). | `./run_pipeline.sh isear bilstm_bert --finetune_mode 3`                                            |
| `./run_pipeline.sh --force_preprocess <dataset> <model> --finetune_mode <1\|2\|3>`             | Combines forced re-preprocessing **and** a specified finetuning strategy.                                        | `./run_pipeline.sh --force_preprocess wassa mamba --finetune_mode 1`                              |

**Notes:**
- **Datasets:** `crowdflower`, `isear`, `wassa`
- **Models:** `bilstm_glove`, `bilstm_bert`, `mamba`
- **Finetune modes:**  
  1. Load checkpoint, freeze encoder, finetune classifier  
  2. Load checkpoint, finetune encoder and classifier  
  3. Train completely from scratch  

`--finetune_mode` is **only required** if you're training on `isear` or `wassa` **with** `mamba` or `bilstm_bert`.

`--force_preprocess` is optional; if omitted, the script will only preprocess if it hasnâ€™t been done before.

---

## ğŸ› ï¸ Model Architectures
- **Baseline:** Fine-tuned BERT classifier.
- **Proposed:** **Selective Mamba** embeddings optimized with **Supervised + Self-Supervised Contrastive Learning** for enhanced emotion discrimination.
- **Metrics:** Accuracy, F1-score, and t-SNE visualizations.

---

## ğŸ“Œ Expected Results - TBC
âœ… **Hypothesis:** The integration of Mamba and Contrastive Learning will yield superior performance and better generalization.

| **Model**                   | **Dataset**     | **Accuracy** | **F1-Score** |
|-----------------------------|-----------------|--------------|--------------|
| Bilstm                      | CrowdFlower     | 85.2%        | 84.8%        |
| Bilstm                      | WASSA           | 78.5%        | 77.9%        |
| Bilstm                      | ISEAR           | 78.5%        | 77.9%        |
| **Contrastive-Mamba (ours)**| **CrowdFlower** | **89.0%**    | **88.5%**    |
| **Contrastive-Mamba (ours)**| **WASSA**       | **85.0%**    | **84.3%**    |
| **Contrastive-Mamba (ours)**| **ISEAR**       | **85.0%**    | **84.3%**    |

---

## ğŸ“Œ To-Do Checklist
- [x] Load and preprocess datasets
- [x] Implement Baseline BERT classifier
- [x] Implement Contrastive Loss
- [x] Implement Mamba Model
- [ ] Fine-tune Contrastive-Mamba Model
- [ ] Evaluate cross-dataset generalization
- [ ] Few-shot evaluation
- [ ] Write and submit final report

---

## ğŸ¤ Contributing
Feel free to contribute! Fork the repo, create a new branch, and submit a pull request.

---

## ğŸ“œ References
- Gu, A., & Dao, T. (2023). "Mamba: Linear-Time Sequence Modeling with Selective State Spaces".
- Gao, T., Yao, X., & Chen, D. (2021). "SimCSE: Simple Contrastive Learning of Sentence Embeddings".
- Gunel, B., et al. (2021). "Supervised Contrastive Learning for Pretrained Language Model Fine-Tuning".
- CrowdFlower Dataset: [Link](https://data.world/crowdflower/sentiment-analysis-in-text)
- WASSA 2017 Dataset: [Link](https://github.com/vinayakumarr/WASSA-2017)

---

## ğŸ† Acknowledgments
This project is part of SC4001 CE/CZ4042: Neural Networks and Deep Learning.

